/* Copyright 2025 The xLLM Authors. All Rights Reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     https://github.com/jd-opensource/xllm/blob/main/LICENSE
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * ==============================================================================
 */

#include <acl/acl.h>
#include <glog/logging.h>
#include <gtest/gtest.h>
#include <torch/torch.h>
#include <torch_npu/torch_npu.h>

#include <optional>

#include "kernel_loader.h"
#include "test_utils.h"
#include "torch_api/triton_ops_api.h"
#include "torch_npu/csrc/core/npu/NPUCachingAllocator.h"

namespace xllm::kernel::npu {

constexpr float kTolerance = 1e-3f;
constexpr float kStateTolerance = 1e-2f;
constexpr int32_t kDeviceId = 0;

torch::Tensor l2norm(const torch::Tensor& x,
                     int64_t dim = -1,
                     float eps = 1e-6f) {
  auto inv_norm = torch::rsqrt((x * x).sum(dim, /*keepdim=*/true) + eps);
  return x * inv_norm;
}

std::pair<torch::Tensor, torch::Tensor> torch_recurrent_gated_delta_rule(
    const torch::Tensor& query,
    const torch::Tensor& key,
    const torch::Tensor& value,
    const torch::Tensor& g,
    const torch::Tensor& beta,
    const std::optional<torch::Tensor>& initial_state,
    bool output_final_state,
    bool use_qk_l2norm_in_kernel) {

    auto initial_dtype = query.scalar_type();
    auto device = query.device();
    torch::Tensor query_norm = query;
    torch::Tensor key_norm = key;

    if (use_qk_l2norm_in_kernel) {
        query_norm = l2norm(query, -1, 1e-6);
        key_norm = l2norm(key, -1, 1e-6);
    }

    auto query_ = query_norm.transpose(1, 2).contiguous().to(torch::kFloat32);
    auto key_ = key_norm.transpose(1, 2).contiguous().to(torch::kFloat32);
    auto value_ = value.transpose(1, 2).contiguous().to(torch::kFloat32);
    auto g_ = g.transpose(1, 2).contiguous().to(torch::kFloat32);
    auto beta_ = beta.transpose(1, 2).contiguous().to(torch::kFloat32);

    int64_t batch_size = key_.size(0);
    int64_t num_heads = key_.size(1);
    int64_t sequence_length = key_.size(2);
    int64_t k_head_dim = key_.size(3);
    int64_t v_head_dim = value_.size(3);

    float scale = 1.0f / std::sqrt(static_cast<float>(k_head_dim));
    query_ = query_ * scale;

    auto core_attn_out = torch::zeros({batch_size, num_heads, sequence_length, v_head_dim}, value_.options());
    torch::Tensor last_recurrent_state;

    if (initial_state.has_value() && initial_state.value().defined()) {
        last_recurrent_state = initial_state.value().to(torch::kFloat32).to(device).contiguous();
    } else {
        last_recurrent_state = torch::zeros({batch_size, num_heads, k_head_dim, v_head_dim}, value_.options());
    }

    for (int64_t i = 0; i < sequence_length; ++i) {
        auto q_t = query_.select(2, i);
        auto k_t = key_.select(2, i);
        auto v_t = value_.select(2, i);
        auto g_t = g_.select(2, i).exp().unsqueeze(-1).unsqueeze(-1);
        auto beta_t = beta_.select(2, i).unsqueeze(-1);

        last_recurrent_state = last_recurrent_state * g_t;
        auto k_t_expanded = k_t.unsqueeze(-1);
        auto kv_mem = (last_recurrent_state * k_t_expanded).sum(-2);
        auto delta = (v_t - kv_mem) * beta_t;
        auto delta_expanded = delta.unsqueeze(-2);
        last_recurrent_state = last_recurrent_state + k_t_expanded * delta_expanded;
        auto q_t_expanded = q_t.unsqueeze(-1);
        auto out_t = (last_recurrent_state * q_t_expanded).sum(-2);
        core_attn_out.slice(2, i, i + 1) = out_t.unsqueeze(2);
    }

    if (!output_final_state) {
        last_recurrent_state = torch::Tensor();
    }

    core_attn_out = core_attn_out.transpose(1, 2).contiguous().to(initial_dtype);
    return std::make_pair(core_attn_out, last_recurrent_state);
}

class TritonRecurrentGatedDeltaRuleTest : public ::testing::Test {
 protected:
  void SetUp() override {
    try {
      torch::zeros({1}, torch::TensorOptions().device("npu:0"));
      tensor_options_ =
          torch::TensorOptions().dtype(torch::kFloat16).device("npu:0");
      npu_available_ = true;
    } catch (...) {
      tensor_options_ =
          torch::TensorOptions().dtype(torch::kFloat16).device(torch::kCPU);
      npu_available_ = false;
      return;
    }

    torch::manual_seed(42);
    torch_npu::init_npu(device_str_);
    kernel_name_ = "fused_recurrent_gated_delta_rule_fwd_kernel";
    binary_filename_ = "fused_recurrent_gated_delta_rule_fwd_kernel.npubin";
    binary_path_ = GetKernelBinaryPath(binary_filename_);
    auto& loader = KernelLoader::get_instance();
    auto handle = loader.get_kernel(kernel_name_);
    if (!handle.is_valid()) {
      handle = loader.load_kernel(kernel_name_, binary_path_);
    }
    ASSERT_TRUE(handle.is_valid()) << "Failed to load Kernel: " << kernel_name_
                                   << " from " << binary_path_;
  }

  void TearDown() override {
    if (npu_available_) {
      try {
        torch_npu::finalize_npu();
      } catch (...) {
      }
    }
  }

  torch::TensorOptions tensor_options_;
  bool npu_available_ = false;
  std::string device_str_ = "npu:" + std::to_string(kDeviceId);
  std::string binary_filename_;
  std::string kernel_name_;
  std::string binary_path_;
};

TEST_F(TritonRecurrentGatedDeltaRuleTest, MultiBatchTest) {
  if (!npu_available_) {
    GTEST_SKIP() << "NPU device not available";
  }

  auto device = at::Device(device_str_);
  constexpr int64_t batch = 4;
  constexpr int64_t T = 1;
  constexpr int64_t num_heads = 4;
  constexpr int64_t num_v_heads = 8;
  constexpr int64_t k_head_dim = 128;
  constexpr int64_t v_head_dim = 128;
  constexpr bool use_qk_l2norm_in_kernel = true;

  torch::manual_seed(0);
  auto dtype = torch::kFloat16;
  auto L = batch * T;
  
  auto q = torch::randn({batch, T, num_heads, k_head_dim}, dtype);
  auto k = torch::randn({batch, T, num_heads, k_head_dim}, dtype);
  auto v = torch::randn({batch, T, num_v_heads, v_head_dim}, dtype);
  auto g = torch::randn({batch, T, num_v_heads}, torch::kFloat32);
  auto beta = torch::randn({batch, T, num_v_heads}, torch::kFloat32);
  auto initial_state = torch::randn({batch, num_v_heads, k_head_dim, v_head_dim}, torch::kFloat32);

  torch::Tensor q_expanded = q, k_expanded = k;
  if (num_v_heads / num_heads > 1) {
      q_expanded = q.repeat_interleave(num_v_heads / num_heads, 2);
      k_expanded = k.repeat_interleave(num_v_heads / num_heads, 2);
  }

  auto [golden_o, golden_state] = torch_recurrent_gated_delta_rule(
    q_expanded, k_expanded, v, g, beta,
    initial_state,
    true,
    true
  );

  // Apply correct reshape operations as in original test
  auto q_d = q.reshape({1, L, num_heads, k_head_dim}).to(device);
  auto k_d = k.reshape({1, L, num_heads, k_head_dim}).to(device);
  auto v_d = v.reshape({1, L, num_v_heads, v_head_dim}).to(device);
  auto g_d = g.reshape({1, L, num_v_heads}).to(device);
  auto beta_d = beta.reshape({1, L, num_v_heads}).to(device);
  auto init_d = initial_state.to(device);

  // Create cu_seqlens [0, 1, 2, ..., batch] as in original test
  std::vector<int64_t> culen;
  culen.reserve(batch + 1);
  for (int i = 0; i <= batch; ++i) {
      culen.push_back(i);
  }
  auto cu_seqlens = torch::tensor(culen, torch::kInt64).to(device);

  // Calculate scale factor
  float scale_val = 1.0f / std::sqrt(static_cast<float>(k_head_dim));
  auto npu_stream = c10_npu::getCurrentNPUStream(kDeviceId);
  
  // Call NPU kernel 
  auto [o_d, state_d] = npu_fused_recurrent_gated_delta_rule(
      q_d, k_d, v_d, g_d,
      beta_d,
      scale_val,
      init_d,
      false,
      cu_seqlens,
      std::nullopt,
      std::nullopt,
      use_qk_l2norm_in_kernel
  );
  aclrtSynchronizeStream(npu_stream.stream());

  // Reshape output to match golden implementation shape
  auto o = o_d.cpu().reshape(golden_o.sizes());
  auto state = state_d.cpu();
  
  // Compare results
  auto output_diff = (golden_o - o).abs().max().item<float>();
  EXPECT_LT(output_diff, 1e-3) 
      << "Output mismatch: max diff = " << output_diff
      << ", shape: " << o.sizes()
      << ", golden range [" << golden_o.min().item<float>() << ", " << golden_o.max().item<float>() << "]"
      << ", actual range [" << o.min().item<float>() << ", " << o.max().item<float>() << "]";

  auto state_diff = (golden_state - state).abs().max().item<float>();
  EXPECT_LT(state_diff, 1e-2)
      << "State mismatch: max diff = " << state_diff
      << ", shape: " << state.sizes()
      << ", golden state range [" << golden_state.min().item<float>() << ", " << golden_state.max().item<float>() << "]"
      << ", actual state range [" << state.min().item<float>() << ", " << state.max().item<float>() << "]";
}

}  // namespace xllm::kernel::npu

int main(int argc, char** argv) {
  ::testing::InitGoogleTest(&argc, argv);

  bool npu_available = false;
  std::string device_str =
      "npu:" + std::to_string(xllm::kernel::npu::kDeviceId);
  try {
    auto test_tensor =
        torch::zeros({1}, torch::TensorOptions().device(device_str));
    (void)test_tensor;
    npu_available = true;
  } catch (...) {
    npu_available = false;
  }

  if (!npu_available) {
    LOG(WARNING) << "NPU device not available, skipping all tests.";
    return 0;
  }

  int result = RUN_ALL_TESTS();
  return result;
}
