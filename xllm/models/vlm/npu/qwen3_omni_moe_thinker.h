/* Copyright 2025 The xLLM Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    https://github.com/jd-opensource/xllm/blob/main/LICENSE

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/

#pragma once

#if defined(USE_NPU)
#include <atb/atb_infer.h>

#include "xllm_kernels/core/include/atb_speed/log.h"
#endif
#include <c10/core/ScalarType.h>
#include <glog/logging.h>
#include <torch/torch.h>

#include <unordered_map>

#include "core/framework/kv_cache/kv_cache.h"
#include "core/framework/model/model_input_params.h"
#include "core/framework/model_context.h"
#include "core/layers/lm_head.h"
#include "core/layers/npu/npu_rms_norm_impl.h"
#include "core/layers/qwen3_vision_encode_layer.h"
#include "core/layers/qwen3_audio_encode_layer.h"
#include "models/llm/npu/qwen3_moe.h"
#include "models/model_registry.h"
#include "processors/input_processor.h"
#include "processors/qwen2_vl_image_processor.h"
#include "processors/feature_extraction_whisper.h"
#include "qwen2_5_vl.h"
#include "qwen3_vl.h"
#include "xllm_kernels/core/include/atb_speed/log.h"
#include "torch_npu/csrc/core/npu/register/OptionRegister.h"

#include "framework/state_dict/state_dict.h"
#include <unistd.h>

namespace xllm {

void SetDefaultAllowInternalFromatDisable()
{
    auto allow_internal_format = c10_npu::option::GetOption("ALLOW_INTERNAL_FORMAT");
    if (allow_internal_format.has_value() && allow_internal_format.value() != "") {
        return;
    }

    c10_npu::option::SetOption("ALLOW_INTERNAL_FORMAT", "disable");
    ASCEND_LOGI("Set ALLOW_INTERNAL_FORMAT default value disable.");
}

#define PrintTensor(tensor) print_tensor(tensor, #tensor, 10, true, false);
std::pair<torch::Tensor, torch::Tensor> get_feat_extract_output_lengths(torch::Tensor input_lengths) {

    auto input_lengths_leave = input_lengths % 100;
    auto feat_lengths = torch::floor_divide((input_lengths_leave - 1), 2) + 1;
    auto output_lengths = torch::floor_divide((torch::floor_divide((feat_lengths - 1), 2) + 1 - 1), 2) + 1 + torch::floor_divide(input_lengths, 100) * 13;
    //auto input_lengths_leave = input_lengths % 100;
    std::cout << input_lengths_leave;
    //auto feat_lengths = (input_lengths_leave - 1) / 2 + 1;
    std::cout << feat_lengths;
    //auto output_lengths = ((feat_lengths - 1) / 2 + 1 - 1) / 2 + 1 + (input_lengths / 100) * 13; 
    std::cout << output_lengths;
    return std::make_pair(feat_lengths, output_lengths);
}


class SinusoidsPositionEmbeddingImpl : public torch::nn::Module {
public:
    SinusoidsPositionEmbeddingImpl(int64_t length, int64_t channels, double max_timescale = 10000.0) {
        if (channels % 2 != 0) {
            CHECK(false)
              << "SinusoidsPositionEmbedding needs even channels input";
        }
        
        double log_timescale_increment = std::log(max_timescale) / (channels / 2 - 1);
        torch::Tensor inv_timescales = torch::exp(-log_timescale_increment * 
            torch::arange(channels / 2)).to(torch::kFloat32);
        
        torch::Tensor scaled_time = torch::arange(length).unsqueeze(1) * inv_timescales.unsqueeze(0);
        
        pos_embedding_ = torch::cat({
            torch::sin(scaled_time),
            torch::cos(scaled_time)
        }, 1);
        
    }
    
    torch::Tensor forward(int64_t seqlen) {
        return pos_embedding_.slice(0, 0, seqlen);
    }
    
private:
   torch::Tensor pos_embedding_;
};

TORCH_MODULE(SinusoidsPositionEmbedding);


class Qwen3_Omni_Moe_Thinker_AudioBlockImpl : public torch::nn::Module {
 public:
  Qwen3_Omni_Moe_Thinker_AudioBlockImpl(const ModelContext& context) {
    // register submodules
    encoder_layer_ = register_module("encoder_layer",
                                     layer::Qwen3AudioEncoderLayer(context));
  }

  torch::Tensor forward(torch::Tensor& x,
                        torch::Tensor& cu_seq_len,
                        std::vector<int>& cu_seq_len_vec,
                        ModelInputParams& input_params,
                        int node_id) {
    return encoder_layer_(x,
                          cu_seq_len,
                          cu_seq_len_vec,
                          input_params,
                          node_id);
  }     

  // load the weight from the checkpoint
  void load_state_dict(const StateDict& state_dict) {
    // call each submodule's load_state_dict function
    encoder_layer_->load_state_dict(state_dict);
  }

#if defined(USE_NPU)
  void verify_loaded_weights(const std::string& prefix) const {
    LOG(INFO) << "start layer verdify";
    encoder_layer_->verify_loaded_weights();
    LOG(INFO) << "end layer modify";
  }
  void merge_loaded_weights() { encoder_layer_->merge_loaded_weights(); }
#endif

 private:
  layer::Qwen3AudioEncoderLayer encoder_layer_{nullptr};
};
TORCH_MODULE(Qwen3_Omni_Moe_Thinker_AudioBlock);


class Qwen3_Omni_Moe_Thinker_AudioTransformerImpl : public torch::nn::Module {
public:
    Qwen3_Omni_Moe_Thinker_AudioTransformerImpl(const ModelContext& context){
        auto model_args = context.get_model_args();
        options_ = context.get_tensor_options();
        auto downsample_hidden_size = model_args.mm_audio_downsample_hidden_size();
        embed_dim_ = model_args.mm_audio_d_model();
        num_mel_bins_ = model_args.mm_audio_num_mel_bins();
        max_source_positions_ = model_args.mm_audio_max_source_positions();
        embed_scale_ = model_args.mm_audio_scale_embedding() ? std::sqrt(embed_dim_) : 1.0;
        n_window_ = model_args.mm_audio_n_window();
        
        positional_embedding_ = register_module(
            "positional_embedding",
            SinusoidsPositionEmbedding(max_source_positions_, embed_dim_)
        );
        
        layers_ = register_module("layers", torch::nn::ModuleList());        
        for (int64_t i = 0; i < model_args.mm_audio_encoder_layers(); ++i) {
            auto layer = Qwen3_Omni_Moe_Thinker_AudioBlock(context);
            layers_->push_back(layer);
        }
        
        ln_post_ = register_module("ln_post", 
            torch::nn::LayerNorm(torch::nn::LayerNormOptions({embed_dim_})
                                      .elementwise_affine(true)));
         
        conv2d1_ = register_module("conv2d1",
            torch::nn::Conv2d(torch::nn::Conv2dOptions(1, downsample_hidden_size, 3)
                .stride(2).padding(1).bias(true)));
        
        conv2d2_ = register_module("conv2d2",
            torch::nn::Conv2d(torch::nn::Conv2dOptions(
                downsample_hidden_size, downsample_hidden_size, 3)
                .stride(2).padding(1).bias(true)));
        
        conv2d3_ = register_module("conv2d3",
            torch::nn::Conv2d(torch::nn::Conv2dOptions(
                downsample_hidden_size, downsample_hidden_size, 3)
                .stride(2).padding(1).bias(true)));
        
        int64_t conv_output_dim = ((((num_mel_bins_ + 1) / 2 + 1) / 2 + 1) / 2);
 
        conv_out_ = register_module("conv_out",
            torch::nn::Linear(
                torch::nn::LinearOptions(downsample_hidden_size * conv_output_dim, embed_dim_)
                    .bias(false)));
        
        proj1_ = register_module("proj1",
            torch::nn::Linear(
                torch::nn::LinearOptions(embed_dim_, embed_dim_)
                    .bias(true)));
        
        
        proj2_ = register_module("proj2",
            torch::nn::Linear(
                torch::nn::LinearOptions(embed_dim_, model_args.mm_audio_output_dim())
                    .bias(true)));
        
        n_window_infer_ = model_args.mm_audio_n_window_infer();
        conv_chunksize_ = model_args.mm_audio_conv_chunksize();
        
    }
    
    torch::Tensor forward(
        const torch::Tensor& input_features,
        const ModelInputParams& input_params,
        const torch::Tensor& feature_lens = torch::Tensor()) {
        
        auto [feat_lengths, aftercnn_lens_calc] = get_feat_extract_output_lengths(feature_lens);
        
        auto chunk_num = torch::ceil(feature_lens / (n_window_ * 2)).to(torch::kLong);
        int64_t total_chunks = chunk_num.sum().item<int64_t>();
        
        auto chunk_lengths = torch::full(
            {total_chunks},
            n_window_ * 2,
            torch::TensorOptions().dtype(torch::kLong).device(feature_lens.device()));
        
        auto padded_chunk_num = torch::nn::functional::pad(chunk_num, torch::nn::functional::PadFuncOptions({1, 0}).value(-1));
        auto tail_chunk_index = padded_chunk_num.cumsum(0).slice(0, 1);
        
        auto remainder = feature_lens % (n_window_ * 2);
        chunk_lengths.index_put_({torch::indexing::TensorIndex(tail_chunk_index)}, remainder);
        chunk_lengths.index_put_({torch::indexing::TensorIndex(chunk_lengths == 0)}, n_window_ * 2);
        
        auto input_t = input_features.t();
        auto chunk_lengths_list = chunk_lengths.to(torch::kCPU).contiguous();
       
        auto chunk_lengths_cpu = chunk_lengths.to(torch::kCPU).contiguous();
        std::cout << chunk_lengths_cpu;    
        at::IntArrayRef split_sizes(
            chunk_lengths_cpu.data_ptr<int64_t>(),
            static_cast<size_t>(chunk_lengths_cpu.size(0))
        );
    
        auto chunk_list = input_t.split_with_sizes(split_sizes, 0); 
        
        auto padded_feature = torch::nn::utils::rnn::pad_sequence(chunk_list, true)
            .transpose(1, 2);
        torch::save(padded_feature, "padded_feature.pt");        
        auto [feat_lengths_cnn ,feature_lens_after_cnn] = get_feat_extract_output_lengths(chunk_lengths.to(torch::kLong));
        LOG(INFO) << 2;   
        std::vector<torch::Tensor> mask_tensors;
        for (int64_t i = 0; i < feature_lens_after_cnn.size(0); ++i) {
            int64_t length = feature_lens_after_cnn[i].item<int64_t>();
            LOG(INFO) << length;
            mask_tensors.push_back(torch::ones(
                {length},
                torch::TensorOptions().dtype(torch::kBool).device(padded_feature.device())));
        }
        LOG(INFO) << 3;
        auto padded_mask_after_cnn = torch::nn::utils::rnn::pad_sequence(mask_tensors, true);
        
        padded_feature = padded_feature.unsqueeze(1);
        
        std::vector<torch::Tensor> padded_embeds;
        int64_t batch_size = padded_feature.size(0);
        LOG(INFO) << 4;
        int64_t counter=0;
        for (int64_t start = 0; start < batch_size; start += conv_chunksize_) {
            int64_t end = std::min(start + conv_chunksize_, batch_size);
            LOG(INFO) << start;
            LOG(INFO) << end;
            
            auto chunk = padded_feature.slice(0, start, end);
            if(counter==0)
               torch::save(conv2d1_(chunk), "conv1.pt");
            auto embed = torch::gelu(conv2d1_(chunk));
            if(counter==0)
               torch::save(embed, "gelu.pt");
            embed = torch::gelu(conv2d2_(embed));
            embed = torch::gelu(conv2d3_(embed));
            if(counter==0)
               torch::save(embed, "final_conv.pt");
            counter+=1;
            
            padded_embeds.push_back(embed);
        }
        LOG(INFO) << 5;
        auto padded_embed = torch::cat(padded_embeds, 0);
        
        auto [b, c, f, t] = std::make_tuple(
            padded_embed.size(0),
            padded_embed.size(1),
            padded_embed.size(2),
            padded_embed.size(3));
        
        auto reshaped = padded_embed.permute({0, 3, 1, 2}).contiguous().view({b, t, c * f});
        padded_embed = conv_out_(reshaped);
        LOG(INFO) << 6;
        auto pos_embed = positional_embedding_->forward(padded_embed.size(1))
            .unsqueeze(0)
            .to(options_.device(), padded_embed.dtype());
        
        padded_embed = padded_embed + pos_embed;
        
        auto hidden_states = padded_embed.masked_select(
            padded_mask_after_cnn.unsqueeze(-1).expand_as(padded_embed))
            .view({-1, padded_embed.size(-1)});
        LOG(INFO) <<  padded_mask_after_cnn.size(-1); 
        auto window_aftercnn = padded_mask_after_cnn.size(-1) * 
            (n_window_infer_ / (n_window_ * 2));
        
        std::vector<int> cu_chunk_lens = {};
        for (int64_t i = 0; i < aftercnn_lens_calc.size(0); ++i) {
            int64_t cnn_len =  static_cast<int>(aftercnn_lens_calc[i].item<long>());
            LOG(INFO) << 8;
            int64_t full_windows = cnn_len / window_aftercnn;
            LOG(INFO) << 9;
            for (int64_t j = 0; j < full_windows; ++j) {
                cu_chunk_lens.push_back(window_aftercnn);
            }
            int64_t remainder = cnn_len % window_aftercnn;
            if (remainder != 0) {
                cu_chunk_lens.push_back(remainder);
            }
        }
        LOG(INFO) << 10;
 
        auto cu_seqlens = torch::tensor(cu_chunk_lens, 
            torch::TensorOptions().device(aftercnn_lens_calc.device()).dtype(torch::kInt32))
            .cumsum(-1).to(torch::kInt32);
        LOG(INFO) << 11;
        ModelInputParams& input_params_new =
           const_cast<ModelInputParams&>(input_params);
        torch::Tensor cu_seqlens_cpu = cu_seqlens.cpu();
        LOG(INFO) << 12;
        std::cout << cu_seqlens;
        std::vector<int> cu_seqlens_vec(
           cu_seqlens_cpu.data_ptr<int>(),  // full seqlen vec
           cu_seqlens_cpu.data_ptr<int>() + cu_seqlens_cpu.numel());
        LOG(INFO) << 13;
        torch::save(hidden_states, "hidden_states.py");
        
        for (int idx = 0; idx < layers_->size(); ++idx) {
            hidden_states = layers_[idx]->as<Qwen3_Omni_Moe_Thinker_AudioBlock>()->forward(hidden_states,
                                   cu_seqlens,
                                   cu_seqlens_vec,
                                   input_params_new,
                                   idx);
        }
        torch::save(hidden_states, "hidden_states_after.py"); 
        
        hidden_states = ln_post_(hidden_states);
        hidden_states = proj1_(hidden_states);
        hidden_states = torch::gelu(hidden_states);
        hidden_states = proj2_(hidden_states);
        
        return hidden_states;
    }
    
    void load_state_dict(const StateDict& state_dict) {
        weight::load_weight(state_dict, "conv_out.weight", conv_out_->weight, is_conv_out_weight_loaded_);
        weight::load_weight(state_dict, "proj1.weight", proj1_->weight, is_proj1_weight_loaded_);
        weight::load_weight(state_dict, "proj1.bias", proj1_->bias, is_proj1_bias_loaded_); 
        weight::load_weight(state_dict, "proj2.weight", proj2_->weight, is_proj2_weight_loaded_);
        weight::load_weight(state_dict, "proj2.bias", proj2_->bias, is_proj2_bias_loaded_);
        
        weight::load_weight(state_dict, "conv2d1.weight", conv2d1_->weight, is_conv2d1_weight_loaded_);
        weight::load_weight(state_dict, "conv2d1.bias", conv2d1_->bias, is_conv2d1_bias_loaded_);
        weight::load_weight(state_dict, "conv2d2.weight", conv2d2_->weight, is_conv2d2_weight_loaded_);
        weight::load_weight(state_dict, "conv2d2.bias", conv2d2_->bias, is_conv2d2_bias_loaded_);
        weight::load_weight(state_dict, "conv2d3.weight", conv2d3_->weight, is_conv2d3_weight_loaded_);
        weight::load_weight(state_dict, "conv2d3.bias", conv2d3_->bias, is_conv2d3_bias_loaded_);
        
        weight::load_weight(state_dict, "ln_post.weight", ln_post_->weight, is_ln_post_weight_loaded_);
        weight::load_weight(state_dict, "ln_post.bias", ln_post_->bias, is_ln_post_bias_loaded_);
        for (size_t idx = 0; idx < layers_->size(); idx++) {
           auto prefix = "layers." + std::to_string(idx) + ".";
           layers_[idx]->as<Qwen3_Omni_Moe_Thinker_AudioBlock>()->load_state_dict(
               state_dict.get_dict_with_prefix(prefix));
        } 
    } 

#if defined(USE_NPU)      
  void verify_loaded_weights(const std::string& prefix) {
    LOG(INFO) << "start transformer modify";
    CHECK(is_conv_out_weight_loaded_)
        << "weight is not loaded for " << "conv_out.weight";
    CHECK(is_proj1_weight_loaded_)
        << "weight is not loaded for " << "proj1.weight";
    CHECK(is_proj1_bias_loaded_)
        << "weight is not loaded for " << "proj1.bias";
    CHECK(is_proj2_weight_loaded_)
        << "weight is not loaded for " << "proj2.weight";
    CHECK(is_proj2_bias_loaded_)
        << "weight is not loaded for " << "proj2.bias";
    
    CHECK(is_conv2d1_weight_loaded_)
        << "weight is not loaded for " << "conv2d1.weight";
    CHECK(is_conv2d1_bias_loaded_)
        << "weight is not loaded for " << "conv2d1.bias";
    CHECK(is_conv2d2_weight_loaded_)
        << "weight is not loaded for " << "conv2d2.weight";
    CHECK(is_conv2d2_bias_loaded_)
        << "weight is not loaded for " << "conv2d2.bias";
    CHECK(is_conv2d3_weight_loaded_)
        << "weight is not loaded for " << "conv2d3.weight";
    CHECK(is_conv2d3_bias_loaded_)
        << "weight is not loaded for " << "conv2d3.bias";
    
    CHECK(is_ln_post_weight_loaded_)
        << "weight is not loaded for " << "ln_post.weight";
    CHECK(is_ln_post_bias_loaded_)
        << "weight is not loaded for " << "ln_post.bias";
    LOG(INFO) << "end transformer modify";
    for (int idx = 0; idx < layers_->size(); ++idx) {
      auto prefix = "layers." + std::to_string(idx) + ".";
      layers_[idx]->as<Qwen3_Omni_Moe_Thinker_AudioBlock>()->verify_loaded_weights(prefix);
    }
  }

  void merge_loaded_weights() {
    for (int idx = 0; idx < layers_->size(); ++idx) {
      layers_[idx]->as<Qwen3_Omni_Moe_Thinker_AudioBlock>()->merge_loaded_weights();
    }
  }
#endif 
private:
    int64_t embed_dim_;
    int64_t num_mel_bins_;
    int64_t max_source_positions_;
    double embed_scale_;
    int64_t n_window_;
    int64_t n_window_infer_;
    int64_t conv_chunksize_;
    torch::TensorOptions options_;
    
    SinusoidsPositionEmbedding positional_embedding_{nullptr};
    torch::nn::ModuleList layers_{nullptr};
    torch::nn::LayerNorm ln_post_{nullptr};
    
    torch::nn::Conv2d conv2d1_{nullptr};
    torch::nn::Conv2d conv2d2_{nullptr};
    torch::nn::Conv2d conv2d3_{nullptr};
    
    torch::nn::Linear conv_out_{nullptr};
    torch::nn::Linear proj1_{nullptr};
    torch::nn::Functional act_{nullptr};
    torch::nn::Linear proj2_{nullptr};
   
    bool is_conv_out_weight_loaded_ = false;
    bool is_conv2d1_weight_loaded_ = false;
    bool is_conv2d1_bias_loaded_ = false;
    bool is_conv2d2_weight_loaded_ = false;
    bool is_conv2d2_bias_loaded_ = false;
    bool is_conv2d3_weight_loaded_ = false;
    bool is_conv2d3_bias_loaded_ = false;
    bool is_ln_post_weight_loaded_ = false;
    bool is_ln_post_bias_loaded_ = false;
    bool is_proj1_weight_loaded_ = false;
    bool is_proj1_bias_loaded_ = false;
    bool is_proj2_weight_loaded_ = false;
    bool is_proj2_bias_loaded_ = false;
};

TORCH_MODULE(Qwen3_Omni_Moe_Thinker_AudioTransformer);



class Qwen3_Omni_Moe_Thinker_VisionPatchEmbedImpl : public torch::nn::Module {
 public:
  Qwen3_Omni_Moe_Thinker_VisionPatchEmbedImpl(const ModelContext& context) {
    auto model_args = context.get_model_args();
    auto options = context.get_tensor_options();

    auto in_features = model_args.mm_num_channels() *
                       model_args.mm_temporal_patch_size() *
                       model_args.mm_patch_size() * model_args.mm_patch_size();

    auto out_features = model_args.mm_hidden_size();

    proj_ = register_module(
        "proj",
        torch::nn::Linear(
            torch::nn::LinearOptions(in_features, out_features).bias(true)));

    proj_->weight.set_data(proj_->weight.to(options));
    proj_->bias.set_data(proj_->bias.to(options));
  }

  torch::Tensor forward(torch::Tensor x) { return proj_(x); }

  void load_state_dict(const StateDict& state_dict) {
    auto weight = state_dict.get_tensor("proj.weight");
    if (weight.defined()) {
      weight = weight.reshape({weight.size(0), -1});
      DCHECK_EQ(proj_->weight.sizes(), weight.sizes())
          << "proj weight size mismatch for " << name();
      proj_->weight.data().copy_(weight);
      proj_weight_loaded_ = true;
    }
    auto bias = state_dict.get_tensor("proj.bias");
    if (bias.defined()) {
      bias = bias.reshape({bias.size(0)});
      DCHECK_EQ(proj_->bias.sizes(), bias.sizes())
          << "proj bias size mismatch for " << name();
      proj_->bias.data().copy_(bias);
      proj_bias_loaded_ = true;
    }
  }

  void verify_loaded_weights(const std::string& prefix) const {
    CHECK(proj_weight_loaded_)
        << "weight is not loaded for " << prefix + "proj.weight";
    CHECK(proj_bias_loaded_)
        << "bias is not loaded for " << prefix + "proj.bias";
  }

 private:
  bool proj_weight_loaded_ = false;
  bool proj_bias_loaded_ = false;
  torch::nn::Linear proj_{nullptr};
};
TORCH_MODULE(Qwen3_Omni_Moe_Thinker_VisionPatchEmbed);

class Qwen3_Omni_Moe_Thinker_VisionBlockImpl : public torch::nn::Module {
 public:
  Qwen3_Omni_Moe_Thinker_VisionBlockImpl(const ModelContext& context) {
    // register submodules
    encoder_layer_ = register_module("encoder_layer",
                                     layer::Qwen3VisionEncoderLayer(context));
  }

  torch::Tensor forward(torch::Tensor& x,
                        torch::Tensor& m_cos_pos,
                        torch::Tensor& m_sin_pos,
                        torch::Tensor& cu_seq_len,
                        std::vector<int>& cu_seq_len_vec,
                        ModelInputParams& input_params,
                        int node_id) {
    return encoder_layer_(x,
                          m_cos_pos,
                          m_sin_pos,
                          cu_seq_len,
                          cu_seq_len_vec,
                          input_params,
                          node_id);
  }

  // load the weight from the checkpoint
  void load_state_dict(const StateDict& state_dict) {
    // call each submodule's load_state_dict function
    encoder_layer_->load_state_dict(state_dict);
  }

#if defined(USE_NPU)
  void verify_loaded_weights(const std::string& prefix) const {
    encoder_layer_->verify_loaded_weights();
  }
  void merge_loaded_weights() { encoder_layer_->merge_loaded_weights(); }
#endif

 private:
  layer::Qwen3VisionEncoderLayer encoder_layer_{nullptr};
};
TORCH_MODULE(Qwen3_Omni_Moe_Thinker_VisionBlock);

class Qwen3_Omni_Moe_Thinker_VisionRotaryEmbeddingImpl : public torch::nn::Module {
 public:
  Qwen3_Omni_Moe_Thinker_VisionRotaryEmbeddingImpl(const ModelContext& context) {
    auto model_args = context.get_model_args();
    auto options = context.get_tensor_options();

    dim_ = model_args.mm_head_dim() / 2;
    theta_ = 10000.0;

    auto opts = options.dtype(torch::kFloat32);
    auto inv_freq =
        1.0 / torch::pow(theta_, torch::arange(0, dim_, 2, opts) / dim_);
    inv_freq_ = register_buffer("inv_freq", inv_freq);
  }

  void update_freqs_cache(int64_t seqlen) {
    if (seqlen <= seq_len_cached_) return;

    seqlen *= 2;
    seq_len_cached_ = seqlen;

    auto options = torch::TensorOptions()
                       .dtype(torch::kFloat32)
                       .device(inv_freq_.device());
    inv_freq_ =
        1.0 / torch::pow(theta_, torch::arange(0, dim_, 2, options) / dim_);
    auto seq = torch::arange(seqlen, options);
    freqs_cached_ = torch::outer(seq, inv_freq_);
  }

  torch::Tensor forward(int seqlen) {
    update_freqs_cache(seqlen);
    return freqs_cached_.slice(0, 0, seqlen);
  }

 private:
  int dim_ = 0;
  double theta_ = 0.0;

  int64_t seq_len_cached_ = 0;
  torch::Tensor inv_freq_;
  torch::Tensor freqs_cached_;
};
TORCH_MODULE(Qwen3_Omni_Moe_Thinker_VisionRotaryEmbedding);

class Qwen3_Omni_Moe_Thinker_VisionPatchMergerImpl : public torch::nn::Module {
 public:
  Qwen3_Omni_Moe_Thinker_VisionPatchMergerImpl(const ModelContext& context,
                              bool use_postshuffle_norm = false) {
    auto model_args = context.get_model_args();
    auto options = context.get_tensor_options();
    auto quant_args = context.get_quant_args();
    auto parallel_args = context.get_parallel_args();
    int64_t d_model = model_args.mm_projection_dim();
    int context_dim = model_args.mm_hidden_size();
    int spatial_merge_size = model_args.mm_spatial_merge_size();
    hidden_size_ =
        context_dim * static_cast<int>(std::pow(spatial_merge_size, 2));
    use_postshuffle_norm_ = use_postshuffle_norm;
    if (use_postshuffle_norm)
      norm_ = register_module(
          "norm",
          torch::nn::LayerNorm(torch::nn::LayerNormOptions({hidden_size_})
                                   .elementwise_affine(true)
                                   .eps(1e-6)));
    else
      norm_ = register_module(
          "norm",
          torch::nn::LayerNorm(torch::nn::LayerNormOptions({context_dim})
                                   .elementwise_affine(true)
                                   .eps(1e-6)));
    norm_->weight.set_data(norm_->weight.to(options));
    norm_->bias.set_data(norm_->bias.to(options));

    auto fc1 = torch::nn::Linear(
        torch::nn::LinearOptions(hidden_size_, hidden_size_).bias(true));
    fc1->weight.set_data(fc1->weight.to(options));
    fc1->bias.set_data(fc1->bias.to(options));
    auto act = torch::nn::GELU();
    auto fc2 = torch::nn::Linear(
        torch::nn::LinearOptions(hidden_size_, d_model).bias(true));
    fc2->weight.set_data(fc2->weight.to(options));
    fc2->bias.set_data(fc2->bias.to(options));
    mlp_ = register_module("mlp", torch::nn::Sequential(fc1, act, fc2));
    layers_ = std::make_tuple(fc1, act, fc2);
  }

  torch::Tensor forward(torch::Tensor x) {
    if (use_postshuffle_norm_)
      x = norm_(x.view({-1, hidden_size_}));
    else
      x = norm_(x).view({-1, hidden_size_});
    return mlp_->forward(x);
  }

  void load_state_dict(const StateDict& state_dict) {
    // norm
    const auto& norm_dict = state_dict.get_dict_with_prefix("ln_q.");
    const auto& norm_weight = norm_dict.get_tensor("weight");
    if (norm_weight.defined()) {
      CHECK_EQ(norm_->weight.sizes(), norm_weight.sizes())
          << "weight size mismatch for " << name();
      norm_->weight.data().copy_(norm_weight);
      is_norm_weight_loaded = true;
    }
    const auto norm_bias = norm_dict.get_tensor("bias");
    if (norm_bias.defined()) {
      CHECK_EQ(norm_->bias.sizes(), norm_bias.sizes())
          << "bias size mismatch for " << name();
      norm_->bias.data().copy_(norm_bias);
      is_norm_bias_loaded = true;
    }

    const auto& fc1_dict = state_dict.get_dict_with_prefix("mlp.0.");
    const auto& fc1_weight = fc1_dict.get_tensor("weight");
    if (fc1_weight.defined()) {
      CHECK_EQ(std::get<0>(layers_)->weight.sizes(), fc1_weight.sizes())
          << "weight size mismatch for " << name();
      std::get<0>(layers_)->weight.data().copy_(fc1_weight);
      is_fc1_weight_loaded = true;
    }
    const auto fc1_bias = fc1_dict.get_tensor("bias");
    if (fc1_bias.defined()) {
      CHECK_EQ(std::get<0>(layers_)->bias.sizes(), fc1_bias.sizes())
          << "bias size mismatch for " << name();
      std::get<0>(layers_)->bias.data().copy_(fc1_bias);
      is_fc1_bias_loaded = true;
    }

    const auto& fc2_dict = state_dict.get_dict_with_prefix("mlp.2.");
    const auto& fc2_weight = fc2_dict.get_tensor("weight");
    if (fc2_weight.defined()) {
      CHECK_EQ(std::get<2>(layers_)->weight.sizes(), fc2_weight.sizes())
          << "weight size mismatch for " << name();
      std::get<2>(layers_)->weight.data().copy_(fc2_weight);
      is_fc2_weight_loaded = true;
    }
    const auto fc2_bias = fc2_dict.get_tensor("bias");
    if (fc2_bias.defined()) {
      CHECK_EQ(std::get<2>(layers_)->bias.sizes(), fc2_bias.sizes())
          << "bias size mismatch for " << name();
      std::get<2>(layers_)->bias.data().copy_(fc2_bias);
      is_fc2_bias_loaded = true;
    }
  }

  void verify_loaded_weights(const std::string& prefix) const {
    CHECK(is_fc1_weight_loaded)
        << "weight is not loaded for " << prefix + "mlp.0." + ".weight";
    CHECK(is_fc1_bias_loaded)
        << "bias is not loaded for " << prefix + "mlp.0." + ".bias";
    CHECK(is_fc2_weight_loaded)
        << "weight is not loaded for " << prefix + "mlp.2." + ".weight";
    CHECK(is_fc2_bias_loaded)
        << "bias is not loaded for " << prefix + "mlp.2." + ".bias";
    CHECK(is_norm_weight_loaded)
        << "weight is not loaded for " << prefix + "ln_q" + ".weight";
    CHECK(is_norm_bias_loaded)
        << "bias is not loaded for " << prefix + "ln_q" + ".bias";
  }

 private:
  int hidden_size_;
  bool use_postshuffle_norm_;
  torch::nn::LayerNorm norm_{nullptr};
  torch::nn::Sequential mlp_{nullptr};
  std::tuple<torch::nn::Linear, torch::nn::GELU, torch::nn::Linear> layers_ = {
      nullptr,
      nullptr,
      nullptr};
  bool is_fc1_weight_loaded = false;
  bool is_fc1_bias_loaded = false;
  bool is_fc2_weight_loaded = false;
  bool is_fc2_bias_loaded = false;
  bool is_norm_weight_loaded = false;
  bool is_norm_bias_loaded = false;
};
TORCH_MODULE(Qwen3_Omni_Moe_Thinker_VisionPatchMerger);

class Qwen3_Omni_Moe_Thinker_VisionTransformerImpl : public torch::nn::Module {
 public:
  Qwen3_Omni_Moe_Thinker_VisionTransformerImpl(const ModelContext& context)
      : options_(context.get_tensor_options()) {
    auto model_args = context.get_model_args();
    hidden_size_ = model_args.mm_hidden_size();
    num_heads_ = model_args.mm_num_attention_heads();
    window_size_ = model_args.mm_window_size();
    patch_size_ = model_args.mm_patch_size();
    spatial_merge_size_ = model_args.mm_spatial_merge_size();
    auto& visual_indexes = model_args.mm_deepstack_visual_indexes();
    deepstack_visual_indexes_.insert(deepstack_visual_indexes_.end(),
                                     visual_indexes.begin(),
                                     visual_indexes.end());
    image_size_ = model_args.mm_image_size();
    spatial_merge_unit_ =
        static_cast<int>(spatial_merge_size_ * spatial_merge_size_);
    num_grid_per_side_ = image_size_ / patch_size_;
    

    patch_embed_ =
        register_module("patch_embed", Qwen3_Omni_Moe_Thinker_VisionPatchEmbed(context));
    rotary_pos_emb_ =
        register_module("rotary_pos_emb", Qwen3_Omni_Moe_Thinker_VisionRotaryEmbedding(context));

    blocks_ = register_module("blocks", torch::nn::ModuleList());
    deepstack_mergers_ =
        register_module("deepstack_mergers", torch::nn::ModuleList());

    emb_ = register_module(
        "embedding",
        torch::nn::Embedding(static_cast<int>(std::pow(num_grid_per_side_, 2)), hidden_size_));
    emb_->weight.set_data(emb_->weight.to(options_));

    merger_ = register_module("merger", Qwen3_Omni_Moe_Thinker_VisionPatchMerger(context));

    for (int32_t idx = 0; idx < model_args.mm_num_hidden_layers(); idx++) {
      auto block = Qwen3_Omni_Moe_Thinker_VisionBlock(context);
      blocks_->push_back(block);
      layers_.push_back(block);
    }
    for (int32_t idx = 0; idx < deepstack_visual_indexes_.size(); idx++) {
      auto merger = Qwen3_Omni_Moe_Thinker_VisionPatchMerger(context, true);
      deepstack_mergers_->push_back(merger);
      deepstack_merger_layers_.push_back(merger);
    }
  }

  torch::Tensor rot_pos_emb(torch::Tensor grid_thw) {
    std::vector<torch::Tensor> pos_ids_vec;
    auto count = grid_thw.sizes()[0];
    pos_ids_vec.reserve(count);

    auto grid_thw_cpu = grid_thw.cpu();
    auto options =
        torch::TensorOptions().dtype(torch::kLong).device(grid_thw.device());

    for (int idx = 0; idx < count; ++idx) {
      auto t = grid_thw_cpu[idx][0].item<int64_t>();
      auto h = grid_thw_cpu[idx][1].item<int64_t>();
      auto w = grid_thw_cpu[idx][2].item<int64_t>();

      auto hpos_ids = torch::arange(h, options).unsqueeze(1).expand({-1, w});
      hpos_ids = hpos_ids
                     .reshape({h / spatial_merge_size_,
                               spatial_merge_size_,
                               w / spatial_merge_size_,
                               spatial_merge_size_})
                     .permute({0, 2, 1, 3})
                     .flatten();

      auto wpos_ids = torch::arange(w, options).unsqueeze(0).expand({h, -1});
      wpos_ids = wpos_ids
                     .reshape({h / spatial_merge_size_,
                               spatial_merge_size_,
                               w / spatial_merge_size_,
                               spatial_merge_size_})
                     .permute({0, 2, 1, 3})
                     .flatten();

      pos_ids_vec.push_back(
          torch::stack({hpos_ids, wpos_ids}, -1).repeat({t, 1}));
    }

    auto pos_ids = torch::cat(pos_ids_vec, 0);
    auto max_grid_size =
        grid_thw
            .index({torch::indexing::Slice(),
                    torch::indexing::Slice(1, torch::indexing::None)})
            .max();

    auto rotary_pos_emb_full = rotary_pos_emb_(max_grid_size.item<int64_t>());
    auto rotary_pos_emb = rotary_pos_emb_full.index({pos_ids}).flatten(1);

    return rotary_pos_emb;
  }

  torch::Tensor fast_pos_embed_interpolate(const torch::Tensor& grid_thw) {
    auto device = grid_thw.device();
    int64_t hidden_dim = hidden_size_;
    int64_t m_size = spatial_merge_size_;

    auto grid_cpu = grid_thw.to(torch::kCPU);
    int64_t count = grid_thw.size(0);

    std::vector<torch::Tensor> outputs;
    outputs.reserve(count);

    for (int64_t idx = 0; idx < count; ++idx) {
      int64_t t = grid_cpu[idx][0].item<int64_t>();
      int64_t h = grid_cpu[idx][1].item<int64_t>();
      int64_t w = grid_cpu[idx][2].item<int64_t>();

      auto h_idxs =
          torch::linspace(
              0, static_cast<float>(num_grid_per_side_ - 1), h, torch::kFloat32)
              .to(device);
      auto w_idxs =
          torch::linspace(
              0, static_cast<float>(num_grid_per_side_ - 1), w, torch::kFloat32)
              .to(device);

      auto h_floor = h_idxs.to(torch::kLong);
      auto w_floor = w_idxs.to(torch::kLong);
      auto h_ceil = torch::clamp(h_floor + 1, 0, num_grid_per_side_ - 1);
      auto w_ceil = torch::clamp(w_floor + 1, 0, num_grid_per_side_ - 1);

      auto dh = h_idxs - h_floor;
      auto dw = w_idxs - w_floor;

      auto mesh_d = torch::meshgrid({dh, dw}, "ij");
      auto dh_grid = mesh_d[0], dw_grid = mesh_d[1];

      auto mesh_floor = torch::meshgrid({h_floor, w_floor}, "ij");
      auto h_floor_grid = mesh_floor[0];
      auto w_floor_grid = mesh_floor[1];

      auto mesh_ceil = torch::meshgrid({h_ceil, w_ceil}, "ij");
      auto h_ceil_grid = mesh_ceil[0];
      auto w_ceil_grid = mesh_ceil[1];

      auto h_floor_grid_idx = h_floor_grid * num_grid_per_side_;
      auto h_ceil_grid_idx = h_ceil_grid * num_grid_per_side_;

      auto w11 = dh_grid * dw_grid;
      auto w10 = dh_grid - w11;
      auto w01 = dw_grid - w11;
      auto w00 = 1.0f - dh_grid - dw_grid + w11;

      auto idx00 = h_floor_grid_idx + w_floor_grid;
      auto idx01 = h_floor_grid_idx + w_ceil_grid;
      auto idx10 = h_ceil_grid_idx + w_floor_grid;
      auto idx11 = h_ceil_grid_idx + w_ceil_grid;

      auto indices = torch::stack({idx00, idx01, idx10, idx11}, 0)
                         .reshape({4, -1})
                         .to(torch::kLong);
      auto weights = torch::stack({w00, w01, w10, w11}, 0)
                         .reshape({4, -1, 1})
                         .to(options_);

      auto embeds = emb_(indices);

      auto combined = (embeds * weights).sum(0);  // [h*w, hidden_dim]

      auto repeated = combined.unsqueeze(0).expand({t, -1, -1}).contiguous();
      repeated = repeated.view(
          {t, h / m_size, m_size, w / m_size, m_size, hidden_dim});
      repeated = repeated.permute({0, 1, 3, 2, 4, 5}).reshape({-1, hidden_dim});

      outputs.push_back(repeated);
    }

    return torch::cat(outputs, 0);
  }

  std::tuple<torch::Tensor, std::vector<torch::Tensor>> forward(
      torch::Tensor hidden_states,
      torch::Tensor grid_thw,  // [batch,thw]
      const ModelInputParams& input_params) {
    hidden_states = patch_embed_(hidden_states);
    auto pos_embeds = fast_pos_embed_interpolate(grid_thw);
    hidden_states = hidden_states + pos_embeds;
    //   compute position embedding
    auto rotary_pos_emb = rot_pos_emb(grid_thw);
    // compute cu_seqlens
    auto cu_seqlens = torch::repeat_interleave(
                          grid_thw.index({torch::indexing::Slice(), 1}) *
                              grid_thw.index({torch::indexing::Slice(), 2}),
                          grid_thw.index({torch::indexing::Slice(), 0}))
                          .cumsum(0, torch::kInt32);
    namespace F = torch::nn::functional;
    cu_seqlens = F::pad(
        cu_seqlens, F::PadFuncOptions({1, 0}).mode(torch::kConstant).value(0));

#if defined(USE_NPU)
    // transformers
    cu_seqlens = torch::diff(cu_seqlens);
#endif

    m_cos = rotary_pos_emb.cos().type_as(hidden_states);
    m_cos = m_cos.repeat({1, 2});
    m_sin = rotary_pos_emb.sin().type_as(hidden_states);
    m_sin = m_sin.repeat({1, 2});

    ModelInputParams& input_params_new =
        const_cast<ModelInputParams&>(input_params);
    torch::Tensor cu_seqlens_cpu = cu_seqlens.cpu();
    std::vector<int> cu_seqlens_vec(
        cu_seqlens_cpu.data_ptr<int>(),  // full seqlen vec
        cu_seqlens_cpu.data_ptr<int>() + cu_seqlens_cpu.numel());
    std::vector<torch::Tensor> deepstack_feature_lists;
    deepstack_feature_lists.reserve(deepstack_visual_indexes_.size());
    for (int idx = 0; idx < blocks_->size(); ++idx) {
      hidden_states = layers_[idx](hidden_states,
                                   m_cos,
                                   m_sin,
                                   cu_seqlens,
                                   cu_seqlens_vec,
                                   input_params_new,
                                   idx);
      auto it = std::find(deepstack_visual_indexes_.begin(),
                          deepstack_visual_indexes_.end(),
                          idx);

      if (it != deepstack_visual_indexes_.end()) {
        int index = std::distance(deepstack_visual_indexes_.begin(), it);
        deepstack_feature_lists.push_back(
            deepstack_merger_layers_[index](hidden_states));
      }
    }
    // adapter
    hidden_states = merger_(hidden_states);
    return std::make_tuple(hidden_states, deepstack_feature_lists);
  }

  void load_state_dict(const StateDict& state_dict) {
    patch_embed_->load_state_dict(
        state_dict.get_dict_with_prefix("patch_embed."));
    for (int idx = 0; idx < layers_.size(); ++idx) {
      layers_[idx]->load_state_dict(state_dict.get_dict_with_prefix(
          "blocks." + std::to_string(idx) + "."));
    }

    merger_->load_state_dict(state_dict.get_dict_with_prefix("merger."));

    for (int idx = 0; idx < deepstack_merger_layers_.size(); ++idx) {
      deepstack_merger_layers_[idx]->load_state_dict(
          state_dict.get_dict_with_prefix("merger_list." +
                                          std::to_string(idx) + "."));
    }

    const auto& emb_dict = state_dict.get_dict_with_prefix("pos_embed.");
    const auto& emb_weight = emb_dict.get_tensor("weight");
    if (emb_weight.defined()) {
      CHECK_EQ(emb_->weight.sizes(), emb_weight.sizes())
          << "weight size mismatch for " << name();
      emb_->weight.data().copy_(emb_weight);
      is_emb_weight_loaded = true;
    }
  }

#if defined(USE_NPU)
  void verify_loaded_weights(const std::string& prefix) const {
    patch_embed_->verify_loaded_weights(prefix + "patch_embed.");
    for (int idx = 0; idx < blocks_->size(); ++idx) {
      layers_[idx]->verify_loaded_weights(prefix + "blocks." +
                                          std::to_string(idx) + ".");
    }
    merger_->verify_loaded_weights(prefix + "merger.");

    for (int idx = 0; idx < deepstack_merger_layers_.size(); ++idx) {
      deepstack_merger_layers_[idx]->verify_loaded_weights(
          "deepstack_merger_list." + std::to_string(idx) + ".");
    }
    CHECK(is_emb_weight_loaded)
        << "weight is not loaded for " << prefix + "" + ".bias";
  }

  void merge_loaded_weights() {
    for (int idx = 0; idx < layers_.size(); ++idx) {
      layers_[idx]->merge_loaded_weights();
    }
  }
#endif

 private:
  int hidden_size_ = 0;
  int num_heads_ = 0;
  int window_size_ = 0;
  int patch_size_ = 0;
  int spatial_merge_size_ = 0;
  std::vector<int64_t> deepstack_visual_indexes_;
  int spatial_merge_unit_ = 0;
  int64_t image_size_ = 0;
  int num_grid_per_side_ = 0;

  Qwen3_Omni_Moe_Thinker_VisionPatchEmbed patch_embed_{nullptr};
  Qwen3_Omni_Moe_Thinker_VisionRotaryEmbedding rotary_pos_emb_{nullptr};
  torch::nn::Embedding emb_{nullptr};

  torch::nn::ModuleList blocks_{nullptr};
  std::vector<Qwen3_Omni_Moe_Thinker_VisionBlock> layers_;

  torch::nn::ModuleList deepstack_mergers_{nullptr};
  std::vector<Qwen3_Omni_Moe_Thinker_VisionPatchMerger> deepstack_merger_layers_;
  Qwen3_Omni_Moe_Thinker_VisionPatchMerger merger_{nullptr};

  torch::Tensor m_cos;
  torch::Tensor m_sin;
  int device_id = 0;
  bool is_emb_weight_loaded = false;
  torch::TensorOptions options_;
};
TORCH_MODULE(Qwen3_Omni_Moe_Thinker_VisionTransformer);


using torch::indexing::None;
using ISlice = torch::indexing::Slice;

class Qwen3_Omni_Moe_Thinker_ForConditionalGenerationImpl : public torch::nn::Module {
 public:
  Qwen3_Omni_Moe_Thinker_ForConditionalGenerationImpl(const ModelContext& context)
      : model_args_(context.get_model_args()),
        options_(context.get_tensor_options()) {
    visual_ = register_module("visual", Qwen3_Omni_Moe_Thinker_VisionTransformer(context));
    audio_tower_ = register_module("audio_tower", Qwen3_Omni_Moe_Thinker_AudioTransformer(context));
    language_model_ =
        register_module("language_model", Qwen3MoeForCausalLM(context));
  }

  torch::Tensor get_audio_features(
        torch::Tensor input_features,
        const ModelInputParams& input_params,
        torch::Tensor feature_attention_mask = torch::Tensor(),
        torch::Tensor audio_feature_lengths = torch::Tensor()
    ) {
        /*
        torch::Tensor feature_lens;
        if (feature_attention_mask.defined()) {
            audio_feature_lengths = torch::sum(feature_attention_mask, 1);
            auto permuted = input_features.permute({0, 2, 1});
            auto bool_mask = feature_attention_mask.to(torch::kBool);
            input_features = permuted.index({bool_mask}).permute({1, 0});
        }
        
        if (audio_feature_lengths.defined()) {
            feature_lens = audio_feature_lengths;
        } else if (feature_attention_mask.defined()) {
            feature_lens = torch::sum(feature_attention_mask, -1);
        }
        */
        auto state_dict = StateDictFromSafeTensor::load("/export/home/shanchenfeng/xllm_build/xllm_qwen_embed/qwen_omni_code/input_featss.pt");
        auto input_featss = torch::ones({128, 290});
        
        bool is_conv_out_weight_loaded_ = false;
        weight::load_weight(*state_dict, "input_feat", input_featss, is_conv_out_weight_loaded_);
        LOG(INFO) << is_conv_out_weight_loaded_;
        input_featss = input_featss.to(options_);
        torch::Tensor feature_lens = torch::tensor({290}).to(options_).to(torch::kLong);
        torch::Tensor audio_features = audio_tower_->forward(input_featss, input_params, feature_lens);
        return audio_features;
    }

  torch::Tensor get_input_embeddings(
      torch::Tensor input_ids,
      const std::optional<Qwen3_VLImageInputs>& image_input,
      const std::optional<Qwen3_VLVideoInputs>& video_input,
      const ModelInputParams& input_params) {
    auto inputs_embeds = language_model_->get_input_embeddings(input_ids);
    if (image_input) {
      // visual
      auto [image_embeds, deep_stacks] =
          visual_(image_input->pixel_values.to(options_),
                  image_input->image_grid_thw,
                  input_params);
      input_params.deep_stacks = deep_stacks;
      // merge
      auto is_multimodal = torch::isin(input_ids, model_args_.image_token_id());
      input_params.visual_pos_masks = is_multimodal;
      inputs_embeds.index_put_({is_multimodal}, image_embeds);
    }
    return inputs_embeds;
  }

  torch::Tensor forward(const torch::Tensor& tokens,
                        const torch::Tensor& positions,
                        std::vector<KVCache>& kv_caches,
                        const ModelInputParams& input_params) {
    torch::NoGradGuard no_grad;
    const auto& mm_data = input_params.mm_data;
    torch::Tensor pixel_values;
    
    //auto audio_features = get_audio_features(torch::Tensor(), input_params, torch::Tensor(), torch::Tensor());

    if (const auto& res = mm_data.get<torch::Tensor>("pixel_values"))
      pixel_values = res.value();

    torch::Tensor image_grid_thw;
    if (const auto& res = mm_data.get<torch::Tensor>("image_grid_thw"))
      image_grid_thw = res.value();
    std::optional<Qwen3_VLImageInputs> image_inputs;
    std::optional<Qwen3_VLVideoInputs> video_inputs;

    if (pixel_values.defined() && image_grid_thw.defined())
      image_inputs = Qwen3_VLImageInputs{pixel_values, image_grid_thw};

    auto inputs_embeds =
        get_input_embeddings(tokens, image_inputs, video_inputs, input_params);
    input_params.input_embedding = inputs_embeds;
    auto emb = language_model_(tokens, positions, kv_caches, input_params);

    return emb;
  }

  torch::Tensor logits(const torch::Tensor& hidden_states,
                       const torch::Tensor& seleted_idxes) {
    return language_model_->logits(hidden_states, seleted_idxes);
  }

  void load_model(std::shared_ptr<ModelLoader> loader) {
    LOG(INFO) << "begin load models";
    for (const auto& state_dict : loader->get_state_dicts()) {
      if(state_dict->get_tensor("thinker.lm_head.weight").defined()){
         state_dict->rename_prefix_inplace("thinker.lm_head.", "lm_head.");
      }
      LOG(INFO) << "load model for visual";
      visual_->load_state_dict(
          state_dict->get_dict_with_prefix("thinker.visual."));
      LOG(INFO) << "load model for audio";
      audio_tower_->load_state_dict(
          state_dict->get_dict_with_prefix("thinker.audio_tower."));
    }
#if defined(USE_NPU)
    // verify
    LOG(INFO) << "start visual verify";
    visual_->verify_loaded_weights("thinker.visual.");
    visual_->merge_loaded_weights();
    LOG(INFO) << "start audio verify";
    audio_tower_->verify_loaded_weights("thinker.audio_tower.");
    audio_tower_->merge_loaded_weights();
    audio_tower_->to(options_.device(), torch::typeMetaToScalarType(options_.dtype()));
    
#endif
    LOG(INFO) << "start load language model";
    if (!model_args_.image_embedding_mode()) {
      language_model_->load_shared_model(loader, "thinker.model.");
    }
  }

  layer::LmHead get_lm_head() { return language_model_->get_lm_head(); }
  void set_lm_head(layer::LmHead& head) { language_model_->set_lm_head(head); }

  layer::WordEmbedding get_word_embedding() {
    return language_model_->get_word_embedding();
  }

  void set_word_embedding(layer::WordEmbedding& word_embedding) {
    language_model_->set_word_embedding(word_embedding);
  }

 private:
  ModelArgs model_args_;
  torch::TensorOptions options_;
  Qwen3_Omni_Moe_Thinker_VisionTransformer visual_{nullptr};
  Qwen3_Omni_Moe_Thinker_AudioTransformer audio_tower_{nullptr};
  Qwen3MoeForCausalLM language_model_{nullptr};
  
};
TORCH_MODULE(Qwen3_Omni_Moe_Thinker_ForConditionalGeneration);

REGISTER_INPUT_PROCESSOR(qwen3_omni_moe_thinker, Qwen2_5_VLInputProcessor);
REGISTER_CAUSAL_VLM_MODEL(qwen3_omni_moe_thinker, Qwen3_Omni_Moe_Thinker_ForConditionalGeneration);
REGISTER_IMAGE_PROCESSOR(qwen3_omni_moe_thinker, Qwen2VLImageProcessor);
REGISTER_FEATURE_EXTRACTOR(qwen3_omni_moe_thinker, WhisperFeatureExtractor);

REGISTER_MODEL_ARGS(qwen3_omni_moe_thinker, [&] {
  LOAD_ARG_OR(model_type, "model_type", "qwen3_omni_moe");
  
  // feature extractor processor parameters 
  LOAD_ARG_OR(has_feature_extractor, "has_feature_extractor", true);
  LOAD_ARG_OR(mm_audio_truncation, "truncation", false);
  LOAD_ARG_OR(mm_audio_padding_strategy, "padding_strategy", 1);
  LOAD_ARG_OR(mm_audio_max_length, "max_length", -1);
  LOAD_ARG_OR(mm_audio_pad_to_multiple_of, "pad_to_multiple_of", -1);
  LOAD_ARG_OR(mm_audio_do_normalize, "do_normalize", false);
  LOAD_ARG_OR(mm_audio_return_token_timestamps, "return_token_timestamps", false);  
  LOAD_ARG_OR(mm_audio_return_attention_mask, "return_attention_mask", true);
  
  // thinker config 
  LOAD_ARG_WITH_PREFIX_JSON("thinker_config", [&]{
      LOAD_ARG_OR_PREFIX(
          vision_start_token_id, "vision_start_token_id", 151652);
      LOAD_ARG_OR_PREFIX(
          vision_end_token_id, "vision_end_token_id", 151653);
      LOAD_ARG_OR_PREFIX(vision_token_id, "vision_token_id", 151654);
      LOAD_ARG_OR_PREFIX(image_token_id, "image_token_id", 151655);
      LOAD_ARG_OR_PREFIX(video_token_id, "video_token_id", 151656);
      LOAD_ARG_OR_PREFIX(audio_token_id, "audio_token_id", 151675);
      LOAD_ARG_OR_PREFIX(dtype, "dtype", "bfloat16"); 
   });
  
  // thinker.text_config
  LOAD_ARG_WITH_PREFIX_JSON("thinker_config.text_config", [&]{
      LOAD_ARG_OR_PREFIX(attention_bias, "attention_bias", false);
      LOAD_ARG_OR_PREFIX(attention_dropout, "attention_dropout", 0.0);
      LOAD_ARG_OR_PREFIX(decoder_sparse_step, "decoder_sparse_step", 1);

      LOAD_ARG_OR_PREFIX(bos_token_id, "bos_token_id", 151643);
      LOAD_ARG_OR_PREFIX(eos_token_id, "eos_token_id", 151645);
      LOAD_ARG_OR_PREFIX(hidden_act, "hidden_act", "silu");
      LOAD_ARG_OR_PREFIX(hidden_size, "hidden_size", 2048);
      LOAD_ARG_OR_PREFIX(intermediate_size, "intermediate_size", 768);
      LOAD_ARG_OR_PREFIX(
          max_position_embeddings, "max_position_embeddings", 65536);
      LOAD_ARG_OR_PREFIX(max_window_layers, "max_window_layers", 28);
      LOAD_ARG_OR_PREFIX(n_heads, "num_attention_heads", 32);
      LOAD_ARG_OR_PREFIX(n_layers, "num_hidden_layers", 48);
      LOAD_ARG_OR_PREFIX(n_kv_heads, "num_key_value_heads", 4);
      LOAD_ARG_OR_PREFIX(rms_norm_eps, "rms_norm_eps", 1e-06);
      LOAD_ARG_OR_PREFIX(sliding_window, "sliding_window", 32768);
      LOAD_ARG_OR_PREFIX(tie_word_embeddings, "tie_word_embeddings", false);
      LOAD_ARG_PREFIX(rope_scaling_mrope_section,
               "rope_scaling.mrope_section");
      LOAD_ARG_OR_PREFIX(initializer_range, "initializer_range", 0.02);
      LOAD_ARG_OR_PREFIX(use_sliding_window, "use_sliding_window", false);
      LOAD_ARG_OR_PREFIX(moe_intermediate_size, "moe_intermediate_size", 768);
      LOAD_ARG_OR_PREFIX(norm_topk_prob, "norm_topk_prob", true);
      LOAD_ARG_OR_PREFIX(num_experts, "num_experts", 128);
      LOAD_ARG_OR_PREFIX(num_experts_per_tok, "num_experts_per_tok", 8);
      LOAD_ARG_OR_FUNC_PREFIX(head_dim, "head_dim", [&] {
        return args->hidden_size() / args->n_heads();
      });
      LOAD_ARG_OR_PREFIX(
         rope_scaling_rope_type, "rope_scaling.type", "mrope");
      LOAD_ARG_PREFIX(rope_scaling_mrope_section,
           "rope_scaling.mrope_section");
      LOAD_ARG_OR_PREFIX(rope_theta, "rope_theta", 1000000.0f);
      LOAD_ARG_OR_PREFIX(vocab_size, "vocab_size", 152064);
   });
  LOG(INFO) << args->rope_scaling_mrope_section().size();
  // thinker.vision_config
  LOAD_ARG_WITH_PREFIX_JSON("thinker_config.vision_config", [&]{
      LOAD_ARG_OR_PREFIX(mm_num_hidden_layers, "depth", 27);
      LOAD_ARG_OR_PREFIX(mm_hidden_act, "hidden_act", "gelu_pytorch_tanh");
      LOAD_ARG_OR_PREFIX(mm_hidden_size, "hidden_size", 1152);
      LOAD_ARG_OR_PREFIX(mm_intermediate_size, "intermediate_size", 4304);
      LOAD_ARG_OR_PREFIX(mm_num_attention_heads, "num_heads", 16);
      LOAD_ARG_OR_PREFIX(mm_num_channels, "in_channels", 3);
      LOAD_ARG_OR_PREFIX(mm_projection_dim, "out_hidden_size", 2048);
      LOAD_ARG_OR_PREFIX(mm_patch_size, "patch_size", 16);
      LOAD_ARG_OR_PREFIX(mm_num_position_embeddings,
                "num_position_embeddings",
                2304);
      LOAD_ARG_OR_PREFIX(mm_spatial_merge_size, "spatial_merge_size", 2);
      LOAD_ARG_PREFIX(mm_deepstack_visual_indexes,
               "deepstack_visual_indexes");
      LOAD_ARG_OR_PREFIX(mm_temporal_patch_size, "temporal_patch_size", 2);
      LOAD_ARG_OR_PREFIX(mm_image_size, "image_size", 768);
  

      LOAD_ARG_OR_FUNC_PREFIX(mm_head_dim, "head_dim", [&] {
        return args->mm_hidden_size() / args->mm_num_attention_heads();
      });
  });
  
  // thinker.audio_config
  LOAD_ARG_WITH_PREFIX_JSON("thinker_config.audio_config", [&]{
      LOAD_ARG_OR_PREFIX(mm_audio_num_attention_heads, "encoder_attention_heads", 20);
      LOAD_ARG_OR_PREFIX(mm_audio_hidden_size, "d_model", 1280);
      LOAD_ARG_OR_PREFIX(mm_audio_downsample_hidden_size, "downsample_hidden_size", 480);
      LOAD_ARG_OR_PREFIX(mm_audio_d_model, "d_model", 1280);
      LOAD_ARG_OR_PREFIX(mm_audio_num_mel_bins, "num_mel_bins", 128);
      LOAD_ARG_OR_PREFIX(mm_audio_max_source_positions, "max_source_positions", 1500);
      LOAD_ARG_OR_PREFIX(mm_audio_scale_embedding, "scale_embedding", false);
      LOAD_ARG_OR_PREFIX(mm_audio_n_window, "n_window", 50);
      LOAD_ARG_OR_PREFIX(mm_audio_n_window_infer, "n_window_infer", 800);
      LOAD_ARG_OR_PREFIX(mm_audio_conv_chunksize, "conv_chunksize", 500);
      //LOAD_ARG_OR_PREFIX(mm_audio_encoder_layers, "encoder_layers", 32);
      LOAD_ARG_OR_PREFIX(mm_audio_encoder_layers, "encoder_layers", 0);
      LOAD_ARG_OR_PREFIX(mm_audio_output_dim, "output_dim", 2048);
  });
  LOG(INFO) << std::to_string(args->mm_audio_encoder_layers());
});
}  // namespace xllm
